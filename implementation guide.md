Here is the comprehensive technical documentation for the **Holographic Cognitive Memory System (HCMS)**.

---

# RAIVEN: Holographic Cognitive Memory System (HCMS)
### Architecture & Implementation Guide

**Version:** 1.0.0
**Date:** December 2025
**Backend:** Neo4j 5.x + Python 3.10+

---

## 1. System Overview

The **HCMS** is a hybrid memory architecture designed to provide Large Language Models (LLMs) with long-term, structured, and abstractive memory. Unlike standard RAG (Retrieval Augmented Generation) which relies solely on flat vector similarity, HCMS utilizes a **GraphRAG** approach combined with **RAPTOR** (Recursive Abstractive Processing).

### Core Philosophy
The system mimics biological memory processes:
1.  **Hippocampus (Ingestion):** Fast, chronological storage of raw interactions (Episodic Memory).
2.  **Neocortex (Consolidation):** Background processes that extract facts (Entities) and form higher-level concepts (Summaries).
3.  **Holographic Retrieval:** Queries hit the memory at three levels simultaneously: *Specific Details*, *Abstract Concepts*, and *Relational Facts*.

### The Data Model
The database schema consists of three primary node types living in the same vector space:

*   **`:Chunk`**: A raw segment of conversation text.
    *   *Properties:* `text`, `timestamp`, `embedding`, `role`.
*   **`:Entity`**: A specific noun, concept, or person (e.g., "Python", "Project Omega").
    *   *Properties:* `name` (Unique).
*   **`:Summary`**: An aggregation of multiple chunks, generated by the RAPTOR process.
    *   *Properties:* `text`, `embedding`, `level` (depth of abstraction).

---

## 2. Infrastructure Setup

This system requires a **Neo4j** database (Version 5.11 or higher is mandatory for Vector Index support).

### A. Database (Docker)
We recommend running Neo4j via Docker. This configuration enables the APOC library (useful for utility functions) and exposes the necessary ports.

**`docker-compose.yml`**
```yaml
version: '3.8'
services:
  neo4j:
    image: neo4j:5.15.0
    container_name: hcms_memory
    ports:
      - "7474:7474" # HTTP (Browser)
      - "7687:7687" # Bolt (Application)
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_apoc_export_file_enabled=true
      - NEO4J_apoc_import_file_enabled=true
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_PLUGINS=["apoc"]
    volumes:
      - ./data:/data
      - ./logs:/logs
```

Run the container:
```bash
docker-compose up -d
```

### B. Python Environment
Create a virtual environment and install the required drivers and AI libraries.

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install neo4j sentence-transformers numpy
```

---

## 3. Configuration & Initialization

The system is designed to be self-initializing. Upon the first run, the `CognitiveMemory` class attempts to create the necessary Vector Indexes and Constraints.

### Vector Configuration
By default, the system uses the `all-MiniLM-L6-v2` model. This model outputs vectors with **384 dimensions**.

If you change the embedding model (e.g., to OpenAI `text-embedding-3-small` which is 1536 dim), you **must** update the index creation Cypher query in `_initialize_schema`.

**In `code.py`:**
```python
# Change this if using OpenAI or a larger local model
VECTOR_DIMENSIONS = 384 

session.run(f"""
    CREATE VECTOR INDEX chunk_embeddings IF NOT EXISTS
    FOR (c:Chunk) ON (c.embedding)
    OPTIONS {{indexConfig: {{
        `vector.dimensions`: {VECTOR_DIMENSIONS},
        `vector.similarity_function`: 'cosine'
    }}}}
""")
```

---

## 4. Usage Guide

### A. Ingesting Memories
The entry point for memory is the `add_memory` method. This function performs a **synchronous** write to the graph.

```python
from memory_system import CognitiveMemory

brain = CognitiveMemory()

# 1. User Input
brain.add_memory("I am deploying the server to AWS us-east-1.", role="user")

# 2. AI Response (It is crucial to store the AI's own output too)
brain.add_memory("I have updated the terraform config for AWS.", role="assistant")
```

**What happens internally:**
1.  Text is converted to a vector.
2.  A `:Chunk` node is created.
3.  **Entity Extraction** runs (currently heuristic-based).
4.  `:Chunk` is linked to `:Entity` nodes via `[:MENTIONS]`.
5.  Entities form connections with each other (`[:RELATED_TO]`) based on co-occurrence.

### B. The RAPTOR Process (Abstraction)
In the provided implementation, RAPTOR is triggered explicitly via `_update_raptor_tree`. In a high-load production environment, this should be decoupled.

**Current Logic:**
*   It looks for "Orphan Chunks" (chunks with no parent Summary).
*   It clusters them (groups of 5 in the demo).
*   It generates a summary and creates a `:Summary` node.
*   It creates a `[:SUMMARIZES]` relationship.

**Production Recommendation:**
Move `_update_raptor_tree` to a Celery worker or a Cron job that runs every 10 minutes.

### C. Retrieval
To recall memory, use the `retrieve` method.

```python
context = brain.retrieve("Where is the server located?")

print(context['raptor_summary'])  # "User discussed AWS deployment details..."
print(context['episodic_hits'])   # "I am deploying the server to AWS us-east-1."
print(context['knowledge_graph']) # "AWS is related to us-east-1"
```

---

## 5. Architectural Deep Dive

### Layer 1: The Episodic Stream (Vector Store)
This works like a standard Vector DB (Pinecone/Weaviate). It answers **"What specifically was said?"**
*   **Query Mechanism:** `db.index.vector.queryNodes('chunk_embeddings', ...)`
*   **Use Case:** Exact quote retrieval, recent conversation history.

### Layer 2: The Semantic Web (Knowledge Graph)
This utilizes the graph structure. It answers **"What are the facts?"**
*   **Query Mechanism:** Cypher Traversal.
*   **Logic:**
    1.  Extract keywords from query (e.g., "AWS").
    2.  Find `:Entity` node `{name: "AWS"}`.
    3.  Traverse `[:RELATED_TO]` edges to find neighbors (e.g., "Terraform", "us-east-1").
    4.  Return these relationships as text.
*   **Benefit:** This finds context even if the vector similarity is low (e.g., finding "Terraform" when asking about "AWS" because they are structurally linked in the graph).

### Layer 3: Recursive Abstraction (RAPTOR)
This is the "Tree" structure. It answers **"What is the bigger picture?"**
*   **Query Mechanism:** Vector search against `:Summary` nodes.
*   **Logic:** Summary nodes contain condensed information. A vector search here matches the *intent* of the conversation rather than specific keywords.

---

## 6. Customization & Scaling

### Improving Entity Extraction
The provided code uses a heuristic (capitalized words) for demo purposes. To make this production-ready, replace the logic in `add_memory`:

**Current:**
```python
extracted_entities = [word for word in text.split() if word[0].isupper()]
```

**Recommended (using SpaCy or LLM):**
```python
# Pseudocode using an LLM
prompt = f"Extract all technical entities from this text as a JSON list: {text}"
extracted_entities = json.loads(llm.generate(prompt))
```

### Clustering for RAPTOR
The demo groups chunks sequentially. For better summaries, implement **Leiden Clustering** using Neo4j's Graph Data Science (GDS) library:

```cypher
CALL gds.leiden.stream({
  nodeProjection: 'Chunk',
  relationshipProjection: 'RELATED_TO'
})
YIELD nodeId, communityId
RETURN gds.util.asNode(nodeId).text, communityId
```
*Group chunks by `communityId` before summarizing.*

### Visualization
You can visualize your brain's memory by opening the Neo4j Browser (`http://localhost:7474`) and running:

```cypher
MATCH (n) RETURN n LIMIT 100
```
You will see a visual representation of how your Chunks (Blue), Entities (Orange), and Summaries (Green) are interconnected.

---

## 7. Troubleshooting

**Issue: `ClientError: ... Vector Index ...`**
*   **Cause:** You are likely running an older version of Neo4j.
*   **Fix:** Ensure Docker image is `neo4j:5.11` or later.

**Issue: Import errors**
*   **Cause:** Missing libraries.
*   **Fix:** `pip install neo4j sentence-transformers`

**Issue: Search results are irrelevant**
*   **Cause:** The embedding model (`all-MiniLM-L6-v2`) is small and fast but less accurate than OpenAI's ada-002.
*   **Fix:** Swap the `_embed` function to use OpenAI API or a larger HuggingFace model.